Data is stored in blocks.



Determining the number of records per block can be done by computing the following:



`floor(block size / record size) = records per block`



We take the floor of the result as there cannot be 0.## of a record in a block.



Example:



Block size is 1024 bytes (1 kilobyte)

And a the size of a single record is 80 bytes



**How many records are there in a single block?**



`1024 / 80 = 12.8` since the remainder .8 of a record cannot be stored, there can be 12 records per block.



**Note: For this class, the words block and page can be used interchangeably.**



Now let's move over to search times:



How many blocks need to be read to find a record with the following information:



- Table with 250,000 records
- Record size is 50 bytes
- Records are not ordered
  - This type of format is a heap file
    - Upon insertion, records are simply appended after the most recent record
- Block size is 1024 bytes



Let's analyze this question a bit further. Our professor was kind enough to explain in class that a question of this nature will be specific to the type of format/ordering. Since this table/database is a heap file, the only way to find a record is to search block by block, record by record. Therefore we can already assume that the number of blocks that are required to be read is the average number of blocks for any search. This will be the total number of blocks divided by 2 as the best case is it is in the first block, a different search could contain yield the last block, or the result could just be not found at all (still requiring a full search of every block).



So moving forward we are looking for the average number of blocks to be read to find a record. Knowing this information, let's ask another question to solve the question above:



**How many blocks are there?**



Well the block size is 1024 bytes and each record is 50 byes. Using the information we reviewed above this question, we can apply the following formula:



`floor(block size / record size)`



Applying this formula yields us...



`floor(1024 bytes / 50 bytes)`

`floor(20.48)`

`20` records per block



Since there are 250,000 records we can calculate the number of blocks required to store all the data with the following equation:



`number of blocks = ceiling(records / records per block)`



Applying that formula...



`number of blocks = ceiling(250,000 / 20)`

`number of blocks = ceiling(12500)`

`number of blocks = 12500`



Using the information we discussed about the nature of search times of heap files we can find our final answer by taking the `number of blocks / 2`



`12500 / 2 = 6250`



We can see through our calculations that it requires for us to search through 6,250 blocks to find a specific record.



Let's review some additional information discussed in class:



- Heap files
  - Advantages: Insertion of records is quick
    - There isn't any form of computation required for us to insert a record as we just simply append it after the most recent record.
  - Disadvantages: Searching and deleting is slow
    - There isn't any form of sorting for heap files so we must search through every record in the table to determine if a value exists or not.



---



Conversely, sorted files are ordered records and work closely to the opposite of heap files:



- Advantages: Searching and deleting is fast
  - If a record is said to be a specific id and is sorted by that column and we were to search for a given record say `ID = 16`, we wouldn't need to search anymore after an ID of 17 or more as an ID of 16 cannot exist AFTER an ID of 17. This should be somewhat intuitive, but I'm studying and I'd like to attempt to explain the processes at work.
- Disadvantages: Inserting is slow
  - This is primarily as a single insertion could result in many file operations to move the data where the insertion is required to go to a new page. There are cases where if the record existed previously and the spot is marked unused, but I don't believe we need to know the specifics for this class, but it is good to mention in review.



---



Now let's review an example for a sorted file with the following information:



- Table with 250,000 records
- Record size is 50 bytes
- Records are sorted (by one of the attributes)
- The size of a single block is 1024 bytes



How many blocks to we need to read to find a specific record by a given value in a sorted field (ie we are searching by ID and the table is sorted by the ID attribute)



Well, the process is similar to the heap file search so we must calculate the number of records per block:



`records per block = floor(1024 / 50) = floor(20.48) = 20 records per block`



With this information we can again calculate the number of blocks required to store all the records in our given table:



`total blocks = ceiling(total number of records / records per block)`

`total blocks = ceiling(250,000 / 20)`

`total blocks = ceiling(12,500)`

`total blocks = 12,500`



Now that we have all the required information to do our calculation, we must review a concept specific for sorted files. Since we are technically performing a binary search to find a specific record, we can assume the following:



`number of blocks required to read for search = ceiling(log_2_(total blocks in the table)`)



**NOTE: We obviously can't search 0.## of a block (even though it'd be pretty nice if we could), so we must take the ceiling of the result of our initial log computation.**



Using the final piece of the puzzle, we can apply all the information to calculate our final answer:



`number of blocks required to read for search = ceiling(log_2_(12,500))`

`... = ceiling(13.6096404744) ` 

`... = 14`



Therefore, we are required to read 14 blocks (on average) to search for a specific record.



---



# Indexes

`oh boi`



Indexes are:

- sorted files
- contain 2 attributes
  - Search field
    - This is the attribute we wish to search by `(obviously)`
  - Page number
    - This is usually a link/pointer to the block containing the record
- They are sorted by the search field attribute
  - This allows us to use binary search as mentioned in the coverings provided sorted files



Indexes come in two major flavors:



- Primary index
  - These indexes are where the referenced search field is built to search a sorted key field
- Secondary index
  - There indexes are where the referenced search field is built to search any unordered attribute



There are also some important concepts to review about these two types of indexes:



Let's start with secondary indexes first:



First it is important to remember that the number of records for a secondary index table will usually be the same number of records in the table it is referencing (though this is not a fixed constraint). This is a stark contrast in comparison to primary indexes. It's also worth mentioning that since secondary indexes don't require



Now let's go ahead and explain why primary indexes are a bit better/efficient by means of shear record count:



While secondary indexes require **EVERY** instant of a (unique value given the search attribute) record in a given table to also exist in the index, primary indexes have the benefit of only having a single record **PER PAGE**. We'll cover this concept with a few examples if I'm wording this poorly... lol



---



Let's review with an example:



If a secondary index exists on a key field, how many pages will be read to search for a record give the key field value given the following information:



- The table consists of 250,000 records
- Each record is a size of 50 bytes
- Each page is 1024 bytes or a single kilobyte
- The key field size is 10 bytes
- The pointer field size is 5 bytes



Using the knowledge reviewed above, we know that since the key field (search field) of the secondary index will consist of the same number of records as the table it is referencing since the index exists on a key field (this key field will not allow duplicate values).



We can also use the information presented above to determine the size of each secondary index record.



`secondary index record size = key field size + pointer field size`

`secondary index record size = 10 + 5`

`secondary index record size = 15 bytes`



Now that we know the size of a single record, we can determine the number of pages required to store all the data in the secondary index table.



`records per page = floor(page size / record size)`

`records per page = floor(1024 / 15)`

`records per page = floor(68.26666666666667)`

`records per page = 68`



**Again noting that we take the floor because we cannot store 0.## of a record in a page**



Now we can calculate the total number of pages required for the secondary index table:



`pages = ceiling(total number of records / records per page)`

`pages = ceiling(25,000 / 68)`

`pages = ceiling(3676.470588235294)`

`pages = 3677`



Since we know the total number of pages for the secondary index table, we can now calculate the number of pages required to find a given record.



`number of pages required to search = ceiling(log_2_(secondary index table pages))`

`... = ceiling(11.8443134612)`

`... = 12`



![Image result for but wait there more](https://i.imgur.com/IxWYq.png)



If the record exists, we need to load the page of the table the secondary index references.



Therefore:

- if the record exists, we must search 12 + 1 pages on average to find a given record.
- if the record does not exist, we must search 12 pages on average.
  - Press F to give respects



...

